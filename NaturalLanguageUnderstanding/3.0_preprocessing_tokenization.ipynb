{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"python-notes","language":"python","name":"python-notes"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"3.0_preprocessing_tokenization.ipynb","provenance":[{"file_id":"1B4X6AdIAK5XLZqbHFyaz2YWjFeB-2eHx","timestamp":1637839716985}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"XMScLH1XSVIC"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"b--68FV2SSUg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637846842698,"user_tz":-60,"elapsed":848,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"8df7817b-a266-46c6-d0b8-1ae2736bddfa"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"9kmn-QVlhNuP"},"source":["# Tokenization\n","The objective of text tokenization is to break the text into smaller units which are often more linguistically meaningful.\n","\n","These smaller linguistic units are usually easier to deal with computationally and semantically."]},{"cell_type":"markdown","metadata":{"id":"246YGlK3hNuU"},"source":["## Sentence Tokenization\n","\n","- The `sent_tokenize()` function uses an instance of `PunktSentenceTokenizer` from the `ntlk.tokenize.punkt` module. \n","\n","- To process large amount of data, it is recommended to load the pre-trained `PunktSentenceTokenizer` once, and call its `tokenizer()` method for the task."]},{"cell_type":"code","metadata":{"id":"6RmRDhtzhNuV"},"source":["from nltk.tokenize import sent_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05eneeW6hNuW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637847360519,"user_tz":-60,"elapsed":231,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"4e3e4a39-cf02-4083-b823-de50e40501d2"},"source":["sents = 'Deep in the human unconscious is a pervasive need for a logical universe that makes sense. But, the real universe is always one step beyond logic.'\n","for s in sent_tokenize(sents):\n","    print(s+'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Deep in the human unconscious is a pervasive need for a logical universe that makes sense.\n","\n","But, the real universe is always one step beyond logic.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"ALe1exh_hNuc"},"source":["## Word Tokenization\n","Similarly, the `word_tokenize()` function is a wrapper function that calls the `tokenize()` method on a instance of `TreebankWordTokenizer` class."]},{"cell_type":"code","metadata":{"id":"rv4vrMUnhNuc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637847443434,"user_tz":-60,"elapsed":204,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"f1430252-b29a-499d-fcfc-bf0a803f2e09"},"source":["from nltk.tokenize import word_tokenize\n","print(word_tokenize(sents))\n","print(len(word_tokenize(sents)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Deep', 'in', 'the', 'human', 'unconscious', 'is', 'a', 'pervasive', 'need', 'for', 'a', 'logical', 'universe', 'that', 'makes', 'sense', '.', 'But', ',', 'the', 'real', 'universe', 'is', 'always', 'one', 'step', 'beyond', 'logic', '.']\n","29\n"]}]},{"cell_type":"markdown","metadata":{"id":"CVszJb6nhNud"},"source":["To process large amount of data, please create an instance of `TreebankWordTokenizer` and call its `tokenize()` method for more efficient processing. We will get the same results with the following codes as above."]},{"cell_type":"code","metadata":{"id":"30ATPDimhNue","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637847447704,"user_tz":-60,"elapsed":264,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"3affadcf-65a5-4cb9-9722-9ce8031fe93d"},"source":["from nltk.tokenize import TreebankWordTokenizer\n","tokenizer = TreebankWordTokenizer()\n","\n","print(tokenizer.tokenize(sents))\n","print(len(tokenizer.tokenize(sents)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Deep', 'in', 'the', 'human', 'unconscious', 'is', 'a', 'pervasive', 'need', 'for', 'a', 'logical', 'universe', 'that', 'makes', 'sense.', 'But', ',', 'the', 'real', 'universe', 'is', 'always', 'one', 'step', 'beyond', 'logic', '.']\n","28\n"]}]},{"cell_type":"markdown","metadata":{"id":"I7f-Ni5whNue"},"source":["The `nltk` module has implemented other more task-oriented word tokenizers, which differ in terms of their specific handling of the punctuations and contractions."]},{"cell_type":"markdown","metadata":{"id":"opvWu5U5hNue"},"source":["### Comparing different word tokenizers"]},{"cell_type":"markdown","metadata":{"id":"nWrCbkTAhNuf"},"source":["- `WordPunctTokenizer` should split all punctuations into separate tokens. Check if that is true.\n","- `TreebankWordTokenizer` follows the Penn Treebank conventions for word tokenization.\n"]},{"cell_type":"code","metadata":{"id":"V8c73s6JhNuf"},"source":["from nltk.tokenize import WordPunctTokenizer\n","wpt = WordPunctTokenizer()\n","tbwt = TreebankWordTokenizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXjRUyeBTsXX"},"source":["s = \"Life is easy and beautiful, isn't?\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_KPk-wwhNuf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637846862383,"user_tz":-60,"elapsed":210,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"781c4756-d4a7-4f7d-ab01-5dc4fe0acd8c"},"source":["print(wpt.tokenize(s))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Life', 'is', 'easy', 'and', 'beautiful', ',', 'isn', \"'\", 't', '?']\n"]}]},{"cell_type":"code","metadata":{"id":"vyxgOSeFhNug","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637846864585,"user_tz":-60,"elapsed":4,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"0fd191d8-d31a-4dc2-c0b9-490c0331a256"},"source":["print(tbwt.tokenize(s))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Life', 'is', 'easy', 'and', 'beautiful', ',', 'is', \"n't\", '?']\n"]}]},{"cell_type":"markdown","metadata":{"id":"a8jc6h4AiGAA"},"source":["💬 Discuss the differences between the two tokenizers."]},{"cell_type":"markdown","metadata":{"id":"PshtxHUrLWP_"},"source":["## Subword Tokenization with Huggingface\n","\n","* A special token, [SEP], to mark the end of a sentence, or the separation between two sentences\n","* A special token, [CLS], at the beginning of our text. This token is used for classification tasks, but BERT expects it no matter what your application is\n","* Tokens that conform with the fixed vocabulary used in BERT\n","* The Token IDs for the tokens, from BERT’s tokenizer\n"]},{"cell_type":"code","metadata":{"id":"_8cT1fZjWefz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637846880335,"user_tz":-60,"elapsed":4922,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"eac970e9-541a-4d3f-bd44-b2c2c90ab395"},"source":["!pip install tokenizers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tokenizers\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.10.3\n"]}]},{"cell_type":"code","metadata":{"id":"vNHFNTFgZVjj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637846880660,"user_tz":-60,"elapsed":330,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"db578093-6cb9-4fa3-9330-2efbbb8e5853"},"source":["# Get a pre-trained tokenizer\n","!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-25 13:28:00--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.198.200\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.198.200|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 231508 (226K) [text/plain]\n","Saving to: ‘bert-base-uncased-vocab.txt’\n","\n","\r          bert-base   0%[                    ]       0  --.-KB/s               \rbert-base-uncased-v 100%[===================>] 226.08K  --.-KB/s    in 0.08s   \n","\n","2021-11-25 13:28:00 (2.71 MB/s) - ‘bert-base-uncased-vocab.txt’ saved [231508/231508]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"XgrHe2AmZtaO"},"source":["from tokenizers import BertWordPieceTokenizer\n","\n","tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTOHDt62Zu2U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637848451360,"user_tz":-60,"elapsed":3,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"81fa612a-a901-4726-b205-9908412772bb"},"source":["output = tokenizer.encode('''Hello, y'all! How are you 😁 ? It is a great time to learn about word embeddings. I am trying very hard.''')\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=33, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"iWwRHpAfZ_zQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637848454028,"user_tz":-60,"elapsed":237,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"40bde35b-088d-48d6-9730-daf9d8fd7f60"},"source":["print(output.tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', 'it', 'is', 'a', 'great', 'time', 'to', 'learn', 'about', 'word', 'em', '##bed', '##ding', '##s', '.', 'i', 'am', 'trying', 'very', 'hard', '.', '[SEP]']\n"]}]},{"cell_type":"code","metadata":{"id":"rUziBNnPaLw8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637848454663,"user_tz":-60,"elapsed":5,"user":{"displayName":"Bernardo Freire","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03319347076090220391"}},"outputId":"b60927e1-2c43-442d-e402-912276c6801e"},"source":["print(output.ids)\n","# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 100, 1029, 2009, 2003, 1037, 2307, 2051, 2000, 4553, 2055, 2773, 7861, 8270, 4667, 2015, 1012, 1045, 2572, 2667, 2200, 2524, 1012, 102]\n"]}]},{"cell_type":"markdown","metadata":{"id":"DZUmAw0mh8dH"},"source":["💬 Do you see any subwords? Which are they and why?"]}]}