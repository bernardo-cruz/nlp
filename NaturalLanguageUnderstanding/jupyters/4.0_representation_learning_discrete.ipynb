{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"celltoolbar":"Slideshow","kernelspec":{"display_name":"python-notes","language":"python","name":"python-notes"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"4.0_representation_learning_discrete.ipynb","provenance":[{"file_id":"1KB0DVng3AdZZn-8BHhs3pfy2N0eR56ui","timestamp":1638443867224}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"c257RJApxsg_"},"source":["## Setup\n"]},{"cell_type":"code","metadata":{"id":"_QzYc99Nxsg_"},"source":["# import warnings\n","# warnings.filterwarnings('ignore')\n","import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","## Default Style Settings\n","matplotlib.rcParams['figure.dpi'] = 150\n","pd.options.display.max_colwidth = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OErkxLsrileT","executionInfo":{"status":"ok","timestamp":1638433612543,"user_tz":-60,"elapsed":212,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"67574772-5268-4ce3-faf5-34b89f6e63d8"},"source":["# Downloads\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"suze4erWxsg_"},"source":["## Bag-of-Words Representation\n","In `CountVectorizer()`, we can utilize its parameters:\n","\n","- `min_df`: When building the vocabulary, the vectorizer will ignore terms that have a **document frequency** strictly lower than the given threshold. `float` = the parameter represents a proportion of documents; `integer` = absolute counts.\n","\n","- `max_df`: When building the vocabulary, the vectorizer will ignore terms that have a **document frequency** strictly higher than the given threshold (corpus-specific stop words). `float` = the parameter represents a proportion of documents; `integer` = absolute counts.\n","\n","- `max_features` : Build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus.\n","\n","The CountVectorizer will select the words/features/terms which occur the most frequently. It takes absolute values so if you set the ‘max_features = 3’, it will select the 3 most common words in the data.\n","\n","- `ngram_range` : The lower and upper boundary of the range of n-values for different word n-grams. `tuple` (min_n, max_n), default=(1, 1).\n","\n","- `token_pattern`: Regular expression denoting what constitutes a \"token\" in vocabulary. The default regexp select tokens of 2 or more alphanumeric characters (Note: **punctuation** is completely ignored and always treated as a token separator).\n","\n","- `lower` Converts all characters to lowercase before tokenizing. Default is set to true and takes boolean value.\n","\n","- `stop_words`: sklearn built-in stop words list \n","CountVectorizer(stop_words=’english’\n","\n","- `binary`: Binary Term Frequency\n","Binary Term Frequency captures presence (1) or absence (0) of term in document. \n","By setting ‘binary = True’, the CountVectorizer no more takes into consideration the frequency of the term/word. If it occurs it’s set to 1 otherwise 0. By default, binary is set to False. This is usually used when the count of the term/word does not provide useful information to the machine learning model."]},{"cell_type":"markdown","metadata":{"id":"216cMIgqLe2D"},"source":["### Word encodings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNcYLglkHRoZ","executionInfo":{"status":"ok","timestamp":1638433524255,"user_tz":-60,"elapsed":803,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"92bd0538-3a50-40b9-ad41-2d3414d4d867"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","words = ['robot', 'woman', 'man']\n","# BOW features in sparse format\n","# Create vectorizer\n","cv = CountVectorizer(min_df=0., max_df=1.)\n","\n","# Create vector by passing the text corpus into the vectorizer to get back counts\n","cv_matrix = cv.fit_transform(words)\n","cv_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<3x3 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 3 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kjd32memOFg8","executionInfo":{"status":"ok","timestamp":1638433541071,"user_tz":-60,"elapsed":220,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"33771eb8-47f0-4522-86aa-d26d995db40a"},"source":["# Now, we can inspect how our vectorizer vectorized the text. Output a list of words used, and their index in the vectors\n","cv.vocabulary_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'man': 0, 'robot': 1, 'woman': 2}"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKLOneS1OKVl","executionInfo":{"status":"ok","timestamp":1638434205177,"user_tz":-60,"elapsed":315,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"da0889c1-8573-41f3-fd11-d506fdc3ede4"},"source":["# Check out your corpus\n","vocab = cv.get_feature_names_out()\n","vocab"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['man', 'robot', 'woman'], dtype=object)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bKuBzQyOPMK","executionInfo":{"status":"ok","timestamp":1638433555562,"user_tz":-60,"elapsed":241,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"eff05a1d-9062-4a0e-e5f7-c402943a7c78"},"source":["print(cv_matrix) # document, index, count\n","# Read as follows: The document zero, which is 'robot', has the vocabulary item 'robot' indexed by 1 once."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 1)\t1\n","  (1, 2)\t1\n","  (2, 0)\t1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GERHUXx1Pr6R","executionInfo":{"status":"ok","timestamp":1638388535039,"user_tz":-60,"elapsed":7,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"62a0cbda-d42a-45b8-8f3c-099703f8e719"},"source":["cv_matrix = cv_matrix.toarray()\n","print(cv_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 0]\n"," [0 0 1]\n"," [1 0 0]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"v2MLxbadPXDv","executionInfo":{"status":"ok","timestamp":1638433681984,"user_tz":-60,"elapsed":247,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"889f6539-8d6d-41dd-853d-851b93e73f21"},"source":["pd.DataFrame(cv_matrix, columns=vocab) # rows are the documents, coloumns the features"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>man</th>\n","      <th>robot</th>\n","      <th>woman</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   man  robot  woman\n","0    0      1      0\n","1    0      0      1\n","2    1      0      0\n","3    1      0      0"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"5es0epS9QmYB"},"source":["# Try out changing the input words\n","# Add a new word\n","# Add the same word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"ACu-6MTyQ0Bn","executionInfo":{"status":"ok","timestamp":1638433946253,"user_tz":-60,"elapsed":211,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"bd6baa4d-af42-43aa-c986-ecb5ec5410af"},"source":["words_a = ['robot', 'woman', 'man', 'man']\n","cv_matrix = cv.fit_transform(words_a).toarray()\n","pd.DataFrame(cv_matrix, columns=vocab) # rows are the documents, coloumns the features"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>man</th>\n","      <th>robot</th>\n","      <th>woman</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   man  robot  woman\n","0    0      1      0\n","1    0      0      1\n","2    1      0      0\n","3    1      0      0"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAJi9wwcReqw","executionInfo":{"status":"ok","timestamp":1638434039740,"user_tz":-60,"elapsed":220,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"59d948bc-31d9-4503-a942-eeea28653b99"},"source":["words_b = ['robot', 'woman', 'man, man']\n","#cv = CountVectorizer(min_df=0., max_df=1., binary=True)\n","cv_matrix = cv.fit_transform(words_b)\n","print(cv_matrix)\n","\n","# Check out your vocab\n","vocab = cv.get_feature_names_out()\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 1)\t1\n","  (1, 2)\t1\n","  (2, 0)\t2\n","['man' 'robot' 'woman']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"wEEElxGFRzha","executionInfo":{"status":"ok","timestamp":1638434037200,"user_tz":-60,"elapsed":215,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"12613965-42f3-45eb-8b36-38e50a0431f1"},"source":["# Inspect new dataframe\n","cv_matrix = cv.fit_transform(words_b).toarray()\n","pd.DataFrame(cv_matrix, columns=vocab)               # rows are the documents, coloumns the features"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>man</th>\n","      <th>robot</th>\n","      <th>woman</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   man  robot  woman\n","0    0      1      0\n","1    0      0      1\n","2    2      0      0"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"-IzLbUemZBd1"},"source":["### Binarizer \n","In Scikit-Learn, one-hot encoding is implemented with the Binarizer transformer in the preprocessing module. The Binarizer takes only numeric data, so the text data must be transformed into a numeric space using the CountVectorizer ahead of one-hot encoding. The Binarizer class uses a threshold value (0 by default) such that all values of the vector that are less than or equal to the threshold are set to zero, while those that are greater than the threshold are set to 1. Therefore, by default, the Binarizer converts all frequency values to 1 while maintaining the zero-valued frequencies."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"euKaEmoXYSR7","executionInfo":{"status":"ok","timestamp":1638434190358,"user_tz":-60,"elapsed":290,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"83fb710d-c2fc-476d-d40d-a3c523866b96"},"source":["from sklearn.preprocessing import Binarizer\n","\n","cv   = CountVectorizer()\n","\n","cv_matrix = cv.fit_transform(words_b)                 # words_b = ['robot', 'woman', 'man, man']\n","vocab = cv.get_feature_names_out()\n","\n","onehot = Binarizer()\n","oh_matrix = onehot.fit_transform(cv_matrix.toarray())\n","print(oh_matrix)\n","pd.DataFrame(oh_matrix, columns=vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 0]\n"," [0 0 1]\n"," [1 0 0]]\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>man</th>\n","      <th>robot</th>\n","      <th>woman</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   man  robot  woman\n","0    0      1      0\n","1    0      0      1\n","2    1      0      0"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"DLxN8h0-jEyW"},"source":["The .toarray() method is optional; it converts the sparse matrix representation to a dense one. In corpora with large vocabularies, the sparse matrix representation is much better. Note that we could also use CountVectorizer(binary=True) to achieve one-hot encoding in the above, obviating the Binarizer."]},{"cell_type":"markdown","metadata":{"id":"yV9MfZsbxshA"},"source":["### Corpus\n","We use a small play corpus (list of sentences). Each sentence represents a document and has a label (topic). You already know these topics from earlier classes.\n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"llE7eCU-xshA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638434252786,"user_tz":-60,"elapsed":205,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"b87c75f7-a60b-41df-b1cf-4407b0f91031"},"source":["corpus = [\n","    'This is a ripe toasty wine.', \n","    'A roasty, toasty wine with notes of mocha',\n","    'Spiced coconut chicken with coriander and salt.',\n","    'Coriander chicken pasta.',\n","    'Great dress for vacation.',\n","    'Perfect dress, perfect fit!'\n","]\n","labels = [\n","    'wine', 'wine', 'food', 'food', 'clothing', 'clothing']\n","\n","corpus, labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['This is a ripe toasty wine.',\n","  'A roasty, toasty wine with notes of mocha',\n","  'Spiced coconut chicken with coriander and salt.',\n","  'Coriander chicken pasta.',\n","  'Great dress for vacation.',\n","  'Perfect dress, perfect fit!'],\n"," ['wine', 'wine', 'food', 'food', 'clothing', 'clothing'])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"3Qb-oE4shWIM"},"source":["# List to np.array\n","corpus = np.array(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"1qMLE_T2gyip","executionInfo":{"status":"ok","timestamp":1638434257531,"user_tz":-60,"elapsed":218,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"955ccdeb-b4c7-45a2-f821-f174f0bb9180"},"source":["# Create dataframe\n","corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n","corpus_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>This is a ripe toasty wine.</td>\n","      <td>wine</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A roasty, toasty wine with notes of mocha</td>\n","      <td>wine</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Spiced coconut chicken with coriander and salt.</td>\n","      <td>food</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Coriander chicken pasta.</td>\n","      <td>food</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Great dress for vacation.</td>\n","      <td>clothing</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Perfect dress, perfect fit!</td>\n","      <td>clothing</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          Document  Category\n","0                      This is a ripe toasty wine.      wine\n","1        A roasty, toasty wine with notes of mocha      wine\n","2  Spiced coconut chicken with coriander and salt.      food\n","3                         Coriander chicken pasta.      food\n","4                        Great dress for vacation.  clothing\n","5                      Perfect dress, perfect fit!  clothing"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"0_Hz13EwxshB"},"source":["### Text Preprocessing\n","\n","- Remove special characters\n","- Normalize letter case\n","- Remove redundant spaces\n","- Tokenize each document into word-tokens\n","- Remove stop words\n","- All these preprocessing steps are wrapped in one function, `preprocess_document()`."]},{"cell_type":"code","metadata":{"id":"Wq5x61PhxshB"},"source":["wpt = nltk.WordPunctTokenizer()\n","stop_words = nltk.corpus.stopwords.words('english')\n","\n","def preprocess_document(doc):\n","    # lower case and remove special characters\\whitespaces\n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n","    doc = doc.lower()\n","    doc = doc.strip()\n","    # tokenize document\n","    tokens = wpt.tokenize(doc)\n","    # filter stopwords out of document\n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    # re-create document from filtered tokens\n","    doc = ' '.join(filtered_tokens)\n","    return doc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"haWXC4H8kOmc","executionInfo":{"status":"ok","timestamp":1638434350500,"user_tz":-60,"elapsed":211,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"3107b3fd-10d8-4d51-cc5d-4686a5602d1f"},"source":["# Apply the preprocessing function with a random sentence first\n","doc_luzern = \"Luzern is a beautiful town with a lot of nice places to visit.\"\n","preprocess_document(doc_luzern)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'luzern beautiful town lot nice places visit'"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"FGzFegejk447","executionInfo":{"status":"ok","timestamp":1638434353719,"user_tz":-60,"elapsed":214,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"5e7da09e-6606-436a-afb5-b550edf80bb1"},"source":["# Let's go back to our original play corpus now and preprocess the first document: 'This is a ripe toasty wine.'\n","preprocess_document(corpus[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ripe toasty wine'"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"RhdBOaa4TOMv"},"source":["What elements have been removed via preprocessing? What other preprocessing steps can you notice here?"]},{"cell_type":"code","metadata":{"id":"4A6i0LuFiNTq"},"source":["normalize_corpus = np.vectorize(preprocess_document) ## The `vectorize` function is provided primarily for convenience, not for performance. The implementation is essentially a for loop."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MS4oCBavxshB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638434461802,"user_tz":-60,"elapsed":203,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"f13d3b98-4316-4a4e-934c-05a016a6fbc3"},"source":["# Preprocess the small corpus\n","norm_corpus = normalize_corpus(corpus)\n","print(corpus)\n","print(\"=\"*50)\n","print(norm_corpus)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['This is a ripe toasty wine.' 'A roasty, toasty wine with notes of mocha'\n"," 'Spiced coconut chicken with coriander and salt.'\n"," 'Coriander chicken pasta.' 'Great dress for vacation.'\n"," 'Perfect dress, perfect fit!']\n","==================================================\n","['ripe toasty wine' 'roasty toasty wine notes mocha'\n"," 'spiced coconut chicken coriander salt' 'coriander chicken pasta'\n"," 'great dress vacation' 'perfect dress perfect fit']\n"]}]},{"cell_type":"markdown","metadata":{"id":"18mrrB6lxshC"},"source":["### `CountVectorizer()` from `sklearn`\n","\n","Learn:\n","https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e"]},{"cell_type":"code","metadata":{"id":"xJBOuvXwxshC"},"source":["from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nID1nMTl1lh"},"source":["# BOW features in sparse format\n","# Create vectorizer\n","cv = CountVectorizer(min_df=0., max_df=1.)\n","\n","# Create vector by passing the text corpus into the vectorizer to get back counts\n","cv_matrix = cv.fit_transform(norm_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Ag36FBrp4Nl","executionInfo":{"status":"ok","timestamp":1638434488515,"user_tz":-60,"elapsed":216,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"ff0a1096-2a0c-45fc-a6ba-342066b37f45"},"source":["# Now, we can inspect how our vectorizer vectorized the text. Output a list of words used, and their index in the vectors\n","cv.vocabulary_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'chicken': 0,\n"," 'coconut': 1,\n"," 'coriander': 2,\n"," 'dress': 3,\n"," 'fit': 4,\n"," 'great': 5,\n"," 'mocha': 6,\n"," 'notes': 7,\n"," 'pasta': 8,\n"," 'perfect': 9,\n"," 'ripe': 10,\n"," 'roasty': 11,\n"," 'salt': 12,\n"," 'spiced': 13,\n"," 'toasty': 14,\n"," 'vacation': 15,\n"," 'wine': 16}"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WzYopqjEn9uh","executionInfo":{"status":"ok","timestamp":1638434490571,"user_tz":-60,"elapsed":195,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"dd5a372b-8a05-4d89-bca8-d63423b63e77"},"source":["len(norm_corpus)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3V3oFz8mfOL","executionInfo":{"status":"ok","timestamp":1638435557046,"user_tz":-60,"elapsed":233,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"556fa97b-7326-4e51-8e8c-776858555e26"},"source":["# Check out your corpus' vocab only\n","vocab = cv.get_feature_names_out()\n","vocab"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['chicken', 'coconut', 'coriander', 'dress', 'fit', 'great',\n","       'mocha', 'notes', 'pasta', 'perfect', 'ripe', 'roasty', 'salt',\n","       'spiced', 'toasty', 'vacation', 'wine'], dtype=object)"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyq-LCCLobpa","executionInfo":{"status":"ok","timestamp":1638434494664,"user_tz":-60,"elapsed":218,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"51466f88-083e-44e4-92dc-b5943d9e857c"},"source":["len(vocab)\n","# Dimensionality equals the size of vocab"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["17"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"j3-2mvBzxshC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638434495826,"user_tz":-60,"elapsed":204,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"bd967e3a-3623-4ee0-d30b-f6ff759505ea"},"source":["# Non-zero feature positions in the sparse matrix. Which document has which vocabulary item and how many times.\n","print(cv_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 10)\t1\n","  (0, 14)\t1\n","  (0, 16)\t1\n","  (1, 14)\t1\n","  (1, 16)\t1\n","  (1, 11)\t1\n","  (1, 7)\t1\n","  (1, 6)\t1\n","  (2, 13)\t1\n","  (2, 1)\t1\n","  (2, 0)\t1\n","  (2, 2)\t1\n","  (2, 12)\t1\n","  (3, 0)\t1\n","  (3, 2)\t1\n","  (3, 8)\t1\n","  (4, 5)\t1\n","  (4, 3)\t1\n","  (4, 15)\t1\n","  (5, 3)\t1\n","  (5, 9)\t2\n","  (5, 4)\t1\n"]}]},{"cell_type":"code","metadata":{"id":"6VEu7rM8xshC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638434497970,"user_tz":-60,"elapsed":195,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"c00a4e68-d238-4557-ed0a-6a7588aa4450"},"source":["# View representation\n","cv_matrix = cv_matrix.toarray()\n","cv_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n","       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1],\n","       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n","       [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n","       [0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0]])"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"wr9Z2lwMxshD","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1638434498997,"user_tz":-60,"elapsed":5,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"1eb63ab4-0d57-4011-df22-b78525e45b3b"},"source":["# Inspect document feature vectors\n","pd.DataFrame(cv_matrix, columns=vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chicken</th>\n","      <th>coconut</th>\n","      <th>coriander</th>\n","      <th>dress</th>\n","      <th>fit</th>\n","      <th>great</th>\n","      <th>mocha</th>\n","      <th>notes</th>\n","      <th>pasta</th>\n","      <th>perfect</th>\n","      <th>ripe</th>\n","      <th>roasty</th>\n","      <th>salt</th>\n","      <th>spiced</th>\n","      <th>toasty</th>\n","      <th>vacation</th>\n","      <th>wine</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   chicken  coconut  coriander  dress  ...  spiced  toasty  vacation  wine\n","0        0        0          0      0  ...       0       1         0     1\n","1        0        0          0      0  ...       0       1         0     1\n","2        1        1          1      0  ...       1       0         0     0\n","3        1        0          1      0  ...       0       0         0     0\n","4        0        0          0      1  ...       0       0         1     0\n","5        0        0          0      1  ...       0       0         0     0\n","\n","[6 rows x 17 columns]"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"JcW7e5NsxshD"},"source":["### Ngrams Representation"]},{"cell_type":"code","metadata":{"id":"qHbEr8PpxshD","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1638434501755,"user_tz":-60,"elapsed":238,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"cda8468e-5607-49f4-cb63-37aa176b72a3"},"source":["# You can set the n-gram range to 1,2 to get unigrams as well as bigrams\n","bv = CountVectorizer(ngram_range=(2, 2))\n","bv_matrix = bv.fit_transform(norm_corpus)\n","\n","bv_matrix = bv_matrix.toarray()\n","vocab = bv.get_feature_names()\n","pd.DataFrame(bv_matrix, columns=vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chicken coriander</th>\n","      <th>chicken pasta</th>\n","      <th>coconut chicken</th>\n","      <th>coriander chicken</th>\n","      <th>coriander salt</th>\n","      <th>dress perfect</th>\n","      <th>dress vacation</th>\n","      <th>great dress</th>\n","      <th>notes mocha</th>\n","      <th>perfect dress</th>\n","      <th>perfect fit</th>\n","      <th>ripe toasty</th>\n","      <th>roasty toasty</th>\n","      <th>spiced coconut</th>\n","      <th>toasty wine</th>\n","      <th>wine notes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   chicken coriander  chicken pasta  ...  toasty wine  wine notes\n","0                  0              0  ...            1           0\n","1                  0              0  ...            1           1\n","2                  1              0  ...            0           0\n","3                  0              1  ...            0           0\n","4                  0              0  ...            0           0\n","5                  0              0  ...            0           0\n","\n","[6 rows x 16 columns]"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"9QaA45bRxshD"},"source":["## Tf-Idf\n","\n"]},{"cell_type":"markdown","metadata":{"id":"La3WJqJRxshE"},"source":["### `TfidfVectorizer()` from `sklearn`"]},{"cell_type":"code","metadata":{"id":"-6EZb5FkxshE","colab":{"base_uri":"https://localhost:8080/","height":275},"executionInfo":{"status":"ok","timestamp":1638434796438,"user_tz":-60,"elapsed":216,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"4e38fb24-e478-46b4-d32c-30575c59071f"},"source":["# Once you have more documents, you can specify more parameters\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tv = TfidfVectorizer(min_df=0.,\n","                     max_df=1.,\n","                     norm='l2',\n","                     use_idf=True,\n","                     smooth_idf=True,\n","                     ngram_range= (1,2))\n","\n","tv_matrix = tv.fit_transform(norm_corpus)\n","\n","tv_matrix = tv_matrix.toarray()\n","vocab = tv.get_feature_names_out()\n","pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chicken</th>\n","      <th>chicken coriander</th>\n","      <th>chicken pasta</th>\n","      <th>coconut</th>\n","      <th>coconut chicken</th>\n","      <th>coriander</th>\n","      <th>coriander chicken</th>\n","      <th>coriander salt</th>\n","      <th>dress</th>\n","      <th>dress perfect</th>\n","      <th>dress vacation</th>\n","      <th>fit</th>\n","      <th>great</th>\n","      <th>great dress</th>\n","      <th>mocha</th>\n","      <th>notes</th>\n","      <th>notes mocha</th>\n","      <th>pasta</th>\n","      <th>perfect</th>\n","      <th>perfect dress</th>\n","      <th>perfect fit</th>\n","      <th>ripe</th>\n","      <th>ripe toasty</th>\n","      <th>roasty</th>\n","      <th>roasty toasty</th>\n","      <th>salt</th>\n","      <th>spiced</th>\n","      <th>spiced coconut</th>\n","      <th>toasty</th>\n","      <th>toasty wine</th>\n","      <th>vacation</th>\n","      <th>wine</th>\n","      <th>wine notes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.41</td>\n","      <td>0.41</td>\n","      <td>0.00</td>\n","      <td>0.41</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.35</td>\n","      <td>0.35</td>\n","      <td>0.35</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.35</td>\n","      <td>0.35</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.29</td>\n","      <td>0.29</td>\n","      <td>0.00</td>\n","      <td>0.29</td>\n","      <td>0.35</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.28</td>\n","      <td>0.35</td>\n","      <td>0.00</td>\n","      <td>0.35</td>\n","      <td>0.35</td>\n","      <td>0.28</td>\n","      <td>0.00</td>\n","      <td>0.35</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.35</td>\n","      <td>0.35</td>\n","      <td>0.35</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.48</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.39</td>\n","      <td>0.48</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.48</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.46</td>\n","      <td>0.00</td>\n","      <td>0.46</td>\n","      <td>0.46</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.46</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.28</td>\n","      <td>0.34</td>\n","      <td>0.00</td>\n","      <td>0.34</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.68</td>\n","      <td>0.34</td>\n","      <td>0.34</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   chicken  chicken coriander  chicken pasta  ...  vacation  wine  wine notes\n","0     0.00               0.00           0.00  ...      0.00  0.41        0.00\n","1     0.00               0.00           0.00  ...      0.00  0.29        0.35\n","2     0.28               0.35           0.00  ...      0.00  0.00        0.00\n","3     0.39               0.00           0.48  ...      0.00  0.00        0.00\n","4     0.00               0.00           0.00  ...      0.46  0.00        0.00\n","5     0.00               0.00           0.00  ...      0.00  0.00        0.00\n","\n","[6 rows x 33 columns]"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"D8xajrcsxshF"},"source":["## Step-by-Step of TF-IDF 🏠\n","\n","The following shows the creation and computation of the tfidf matrix step by step. Please go over the codes on your own to practice the tf-idf calculation"]},{"cell_type":"markdown","metadata":{"id":"OSCe1aYjxshF"},"source":["#### Create Vocabulary Dictionary of the Corpus"]},{"cell_type":"code","metadata":{"id":"eDU0mv_ixshF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638388536139,"user_tz":-60,"elapsed":23,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"d1df48b1-3fda-4871-f8a7-a4baecbfdeb5"},"source":["# get unique words as feature names\n","unique_words = list(\n","    set([word for doc in [doc.split() for doc in norm_corpus]\n","         for word in doc]))\n","\n","# default dict \n","def_feature_dict = {w: 0 for w in unique_words}\n","\n","print('Feature Names:', unique_words)\n","print('Default Feature Dict:', def_feature_dict)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Names: ['dress', 'roasty', 'toasty', 'notes', 'chicken', 'perfect', 'fit', 'mocha', 'coriander', 'ripe', 'coconut', 'pasta', 'vacation', 'spiced', 'salt', 'great', 'wine']\n","Default Feature Dict: {'dress': 0, 'roasty': 0, 'toasty': 0, 'notes': 0, 'chicken': 0, 'perfect': 0, 'fit': 0, 'mocha': 0, 'coriander': 0, 'ripe': 0, 'coconut': 0, 'pasta': 0, 'vacation': 0, 'spiced': 0, 'salt': 0, 'great': 0, 'wine': 0}\n"]}]},{"cell_type":"markdown","metadata":{"id":"PKgujnwKxshF"},"source":["#### Create Document-Word Matrix (Bag-of-Word Frequencies)"]},{"cell_type":"code","metadata":{"id":"UcNcCv9kxshF","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1638388536139,"user_tz":-60,"elapsed":22,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"79f961ca-7fb0-492c-d783-22644c04d53c"},"source":["from collections import Counter\n","# build bag of words features for each document - term frequencies\n","bow_features = []\n","for doc in norm_corpus:\n","    bow_feature_doc = Counter(doc.split())\n","    # initialize default corpus dictionary\n","    all_features = Counter(def_feature_dict) \n","    \n","    # update default dict with current doc words\n","    bow_feature_doc.update(all_features)\n","    \n","    # append cur doc dict\n","    bow_features.append(bow_feature_doc)\n","\n","bow_features = pd.DataFrame(bow_features)\n","bow_features"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ripe</th>\n","      <th>toasty</th>\n","      <th>wine</th>\n","      <th>dress</th>\n","      <th>roasty</th>\n","      <th>notes</th>\n","      <th>chicken</th>\n","      <th>perfect</th>\n","      <th>fit</th>\n","      <th>mocha</th>\n","      <th>coriander</th>\n","      <th>coconut</th>\n","      <th>pasta</th>\n","      <th>vacation</th>\n","      <th>spiced</th>\n","      <th>salt</th>\n","      <th>great</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ripe  toasty  wine  dress  roasty  ...  pasta  vacation  spiced  salt  great\n","0     1       1     1      0       0  ...      0         0       0     0      0\n","1     0       1     1      0       1  ...      0         0       0     0      0\n","2     0       0     0      0       0  ...      0         0       1     1      0\n","3     0       0     0      0       0  ...      1         0       0     0      0\n","4     0       0     0      1       0  ...      0         1       0     0      1\n","5     0       0     0      1       0  ...      0         0       0     0      0\n","\n","[6 rows x 17 columns]"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"zT-sFSLuxshF"},"source":["#### Compute Document Frequency of Words"]},{"cell_type":"code","metadata":{"id":"6sLSh7MjxshF","colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"status":"ok","timestamp":1638388536140,"user_tz":-60,"elapsed":21,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"ea58f45e-0d7e-43bf-9beb-db548906c5b1"},"source":["import scipy.sparse as sp\n","feature_names = list(bow_features.columns)\n","\n","# build the document frequency matrix\n","df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n","# `csc_matrix()` compress `bow_features` into sparse matrix based on columns\n","# `csc_matrix.indices` stores the matrix value indices in each column\n","# `csc_matrix.indptr` stores the accumulative numbers of values from column-0 to the right-most column\n","\n","df = 1 + df  # adding 1 to smoothen idf later\n","\n","# show smoothened document frequencies\n","pd.DataFrame([df], columns=feature_names)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ripe</th>\n","      <th>toasty</th>\n","      <th>wine</th>\n","      <th>dress</th>\n","      <th>roasty</th>\n","      <th>notes</th>\n","      <th>chicken</th>\n","      <th>perfect</th>\n","      <th>fit</th>\n","      <th>mocha</th>\n","      <th>coriander</th>\n","      <th>coconut</th>\n","      <th>pasta</th>\n","      <th>vacation</th>\n","      <th>spiced</th>\n","      <th>salt</th>\n","      <th>great</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ripe  toasty  wine  dress  roasty  ...  pasta  vacation  spiced  salt  great\n","0     2       3     3      3       2  ...      2         2       2     2      2\n","\n","[1 rows x 17 columns]"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"z_GT8975xshF"},"source":["#### Create Inverse Document Frequency of Words"]},{"cell_type":"code","metadata":{"id":"ZPQuFamHxshG","colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"status":"ok","timestamp":1638388536140,"user_tz":-60,"elapsed":20,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"ff40ea70-ae21-4fb0-e7b0-af713aed58c5"},"source":["# compute inverse document frequencies for each term\n","total_docs = 1 + len(norm_corpus)\n","idf = 1.0 + np.log(float(total_docs) / df)\n","\n","# show smoothened idfs\n","pd.DataFrame([np.round(idf, 2)], columns=feature_names)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ripe</th>\n","      <th>toasty</th>\n","      <th>wine</th>\n","      <th>dress</th>\n","      <th>roasty</th>\n","      <th>notes</th>\n","      <th>chicken</th>\n","      <th>perfect</th>\n","      <th>fit</th>\n","      <th>mocha</th>\n","      <th>coriander</th>\n","      <th>coconut</th>\n","      <th>pasta</th>\n","      <th>vacation</th>\n","      <th>spiced</th>\n","      <th>salt</th>\n","      <th>great</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.25</td>\n","      <td>1.85</td>\n","      <td>1.85</td>\n","      <td>1.85</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>1.85</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>1.85</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ripe  toasty  wine  dress  roasty  ...  pasta  vacation  spiced  salt  great\n","0  2.25    1.85  1.85   1.85    2.25  ...   2.25      2.25    2.25  2.25   2.25\n","\n","[1 rows x 17 columns]"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"lTuAz9e-xshG"},"source":["#### Compute Raw TF-IDF for Each Document"]},{"cell_type":"code","metadata":{"id":"7KryHrLgxshG","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1638389147520,"user_tz":-60,"elapsed":417,"user":{"displayName":"hslu clt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00844546079669052102"}},"outputId":"ea6bc07c-f707-4079-acb8-4790634c9395"},"source":["# compute tfidf feature matrix\n","tf = np.array(bow_features, dtype='float64')\n","tfidf = tf * idf  \n","# view raw tfidf feature matrix\n","pd.DataFrame(np.round(tfidf, 2), columns=feature_names)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ripe</th>\n","      <th>toasty</th>\n","      <th>wine</th>\n","      <th>dress</th>\n","      <th>roasty</th>\n","      <th>notes</th>\n","      <th>chicken</th>\n","      <th>perfect</th>\n","      <th>fit</th>\n","      <th>mocha</th>\n","      <th>coriander</th>\n","      <th>coconut</th>\n","      <th>pasta</th>\n","      <th>vacation</th>\n","      <th>spiced</th>\n","      <th>salt</th>\n","      <th>great</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.25</td>\n","      <td>1.85</td>\n","      <td>1.85</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>1.85</td>\n","      <td>0.00</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>2.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.25</td>\n","      <td>2.25</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>0.00</td>\n","      <td>2.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.25</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.85</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>4.51</td>\n","      <td>2.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ripe  toasty  wine  dress  roasty  ...  pasta  vacation  spiced  salt  great\n","0  2.25    1.85  1.85   0.00    0.00  ...   0.00      0.00    0.00  0.00   0.00\n","1  0.00    1.85  1.85   0.00    2.25  ...   0.00      0.00    0.00  0.00   0.00\n","2  0.00    0.00  0.00   0.00    0.00  ...   0.00      0.00    2.25  2.25   0.00\n","3  0.00    0.00  0.00   0.00    0.00  ...   2.25      0.00    0.00  0.00   0.00\n","4  0.00    0.00  0.00   1.85    0.00  ...   0.00      2.25    0.00  0.00   2.25\n","5  0.00    0.00  0.00   1.85    0.00  ...   0.00      0.00    0.00  0.00   0.00\n","\n","[6 rows x 17 columns]"]},"metadata":{},"execution_count":48}]}]}